{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d903133d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Fingerprint Left or Right Hand Prediction\n",
    "\n",
    "## About the data\n",
    "Sokoto Coventry Fingerprint Dataset (SOCOFing) is a biometric fingerprint database designed for academic research purposes. SOCOFing is made up of 6,000 fingerprint images from 600 African subjects and contains unique attributes such as labels for gender, hand and finger name as well as synthetically altered versions with three different levels of alteration for obliteration, central rotation, and z-cut. For a complete formal description and usage policy please refer to the following paper: https://arxiv.org/abs/1807.10609.\n",
    "\n",
    "## About the Demo\n",
    "This notebook was built to demonstrate the value of utilizing AWS SageMaker for Model development/training and Red Hat OpenShift on AWS for Model deployment/serving.\n",
    "\n",
    "## About the notebook\n",
    "The intention of this notebook is to demonstrate steps from data ingestion to model saving that provides an accurate enough model that predicts if a fingerprint comes from a left or right hand. Coupled with other models that accurately predict finger and gender is valuable when matching against other identifiable information.\n",
    "\n",
    "1. *Data Ingestion* [from object storage](#working-with-s3-buckets)\n",
    "1. *Dataset preparation* (infer labels, splitting, augmenting, optimizing)\n",
    "1. *Model Development* from scratch and *Training Strategies* (one device, mirrored, multi-worker mirrored)\n",
    "1. *Model Performance* Hyperparameter Tuning strategies (RandomSearch, Hyperband, BayesianOptimization, Sklearn)\n",
    "1. *Model Serialization* to object storage\n",
    "1. *Prediction Sampling*\n",
    "\n",
    "### Notebook Tested Requirements\n",
    "\n",
    "|Notebook origin|Notebook Customization|Instance Type|Kernel|TensorFlow|Runtime|\n",
    "|:-------|:-------|:-------|:-------|:-------|:-------|\n",
    "|SageMaker Notebook Instances|[from GitHub](https://github.com/redhat-na-ssa/demo-rosa-sagemaker/blob/main/sagemaker/lifecycle-from-github.sh)|ml.m5.4xlarge (vCPU: 16, RAM: 64 GiB)|conda_tensorflow2_p310|2.11|~120 minutes|\n",
    "|SageMaker Notebook Instances|[from GitHub](https://github.com/redhat-na-ssa/demo-rosa-sagemaker/blob/main/sagemaker/lifecycle-from-github.sh)|ml.p3.8xlarge (vCPU: 32, RAM: 244 GiB)|conda_tensorflow2_p310|2.11|~15 minutes|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d4b84",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b296636-0a08-47ba-b960-01cfda1a1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages and frameworks\n",
    "\n",
    "# pkgs installed via notebook lifecycle script\n",
    "#! pip install -U pip --quiet\n",
    "#! pip install -r ../../requirements.txt --quiet\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# debugging code \"Cleanup Called...\" gets displayed if get_logger is not set\n",
    "# the below code suppresses the \"Cleanup Called...\" output\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "# expecting 2.11\n",
    "# if 2.7, than logging errors will show \"Cleanup called...\"\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df79ba-3018-4150-b2a1-d8722577b105",
   "metadata": {},
   "source": [
    "If this is your first time running the notebook, you may need to restart the kernel after the Tensorflow upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00308b1-7eca-46eb-aed9-00e52c043d41",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b492230",
   "metadata": {},
   "source": [
    "## Create local directory structure\n",
    "\n",
    "- scratch: ignored by git; keep data out of git\n",
    "- models: saved models\n",
    "- real: \"real\" simulated samples\n",
    "- tune: hyperparameter trials results\n",
    "- train: decompressed training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089211a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch directory is apart of the .gitignore to ensure it is not committed to git\n",
    "%env SCRATCH=../../scratch\n",
    "! [ -e \"${SCRATCH}\" ] || mkdir -p \"${SCRATCH}\"\n",
    "\n",
    "scratch_path = os.environ.get('SCRATCH', './scratch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p \"${SCRATCH}\"/{models,real,tune,train}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62063b13",
   "metadata": {},
   "source": [
    "## Decompress the data for training\n",
    "\n",
    "This is just a simple way to get the decompressed data into the S3 Bucket that keeps this demo and data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kludge: download dataset from git\n",
    "! git clone https://github.com/redhat-na-ssa/demo-rosa-sagemaker-data.git ${SCRATCH}/.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a1779",
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -Jxf ${SCRATCH}/.raw/left.tar.xz -C \"${SCRATCH}\"/train/ && \\\n",
    "  tar -Jxf ${SCRATCH}/.raw/right.tar.xz -C \"${SCRATCH}\"/train/ && \\\n",
    "  tar -Jxf ${SCRATCH}/.raw/real.tar.xz -C \"${SCRATCH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b906563",
   "metadata": {},
   "source": [
    "### Split the data into Train, Validation and Test\n",
    "\n",
    "Keras utility generates a dataset from image files in a directory and infers the labels based on the parent folder. This utility will return a tf.data.Dataset that yields batches of images from the subdirectories left and right\n",
    "\n",
    "```\n",
    "train/\n",
    "├── left/\n",
    "│   ├── a_image_1.jpg\n",
    "│   └── a_image_2.jpg\n",
    "└── right/\n",
    "    ├── b_image_1.jpg\n",
    "    └── b_image_2.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables for consistency\n",
    "img_height = 96              # desired height\n",
    "img_width = 96               # desired width\n",
    "batch_size = 32              # batch inputs in 32\n",
    "seed_train_validation = 42   # Must be same for train_ds and val_ds\n",
    "validation_split = 0.3       # move 30% of the data into validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd733a6",
   "metadata": {},
   "source": [
    "### Create Train\n",
    "\n",
    "Train is the sample of data used to fit the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f74933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training dataset\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    scratch_path + '/train',\n",
    "    labels='inferred',\n",
    "    label_mode = \"categorical\", \n",
    "    class_names=['left','right'],\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True, \n",
    "    seed=seed_train_validation,\n",
    "    validation_split=validation_split,\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f050970a",
   "metadata": {},
   "source": [
    "### Create Validation\n",
    "\n",
    "Validation is the sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the validation dataset\n",
    "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    scratch_path + '/train',\n",
    "    labels='inferred',\n",
    "    label_mode = \"categorical\", \n",
    "    class_names=['left','right'],\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True, \n",
    "    seed=seed_train_validation,\n",
    "    validation_split=validation_split,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82c281",
   "metadata": {},
   "source": [
    "## Create Test\n",
    "\n",
    "The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd96222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test dataset\n",
    "test_ds = validation_ds.take(16)\n",
    "validation_ds = validation_ds.skip(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d9c9f5",
   "metadata": {},
   "source": [
    "## Print the Dataset batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserves 393 batches training\n",
    "print('70% of data in batches of 32 images for training -->', train_ds.cardinality())\n",
    "# reserves 164 batches validation\n",
    "print('20% of data in batches of 32 images for validating -->', validation_ds.cardinality())\n",
    "# reserves 5 batches testing\n",
    "print('10% of data in batches of 32 images for testing -->', test_ds.cardinality())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e179d3",
   "metadata": {},
   "source": [
    "## Print Inferred Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e7c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the class names inferred from the training dataset\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20166d",
   "metadata": {},
   "source": [
    "## Print Fingerprint Data Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adbd61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first 10 images in the training dataset\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(10):\n",
    "        ax = plt.subplot(5, 5, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"), cmap='gray')\n",
    "        #TODO update labels\n",
    "        #plt.title(int[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7699e",
   "metadata": {},
   "source": [
    "## Apply augmentation\n",
    "\n",
    "When you don't have a large image dataset or when your images are all set in a single direction like ours are, it's a good practice to artificially introduce sample diversity by applying random, yet realistic, transformations to the training images, such as rotation and horizontal flipping. This helps expose the model to different aspects of the training data and reduce over-fitting. \n",
    "\n",
    "Learn more https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e93291",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "\n",
    "  # randomly rotates images during training\n",
    "  tf.keras.layers.RandomRotation(\n",
    "    # a float represented as fraction of 2 Pi, or a tuple of size 2 representing lower and upper bound for rotating clockwise and counter-clockwise. \n",
    "    # A positive values means rotating counter clock-wise, while a negative value means clock-wise. \n",
    "    0.2,\n",
    "      \n",
    "    # Points outside the boundaries of the input are filled according to the given mode (one of {\"constant\", \"reflect\", \"wrap\", \"nearest\"}).\n",
    "    fill_mode='constant',\n",
    "      \n",
    "    # Supported values: \"nearest\", \"bilinear\".\n",
    "    interpolation='nearest',\n",
    "      \n",
    "    # Integer. Used to create a random seed.\n",
    "    seed=None,\n",
    "      \n",
    "    # the value to be filled outside the boundaries when fill_mode=\"constant\".\n",
    "    fill_value=0.0,\n",
    "),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a748ba6",
   "metadata": {},
   "source": [
    "Visualize a few augmented examples by applying data augmentation to the same image several times:Visualize a few augmented examples by applying data augmentation to the same image several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b89acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, _ in train_ds.take(1):\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  first_image = image[2]\n",
    "  for i in range(10):\n",
    "    ax = plt.subplot(5, 5, i + 1)\n",
    "    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "    plt.imshow(augmented_image[0] / 1, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec6df2",
   "metadata": {},
   "source": [
    "## Configure the datasets for performance\n",
    "\n",
    "Let's make sure to use buffered prefetching so we can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data.\n",
    "\n",
    "1. `Caching` a dataset, either in memory or on local storage. This will save some operations (like file opening and data reading) from being executed during each epoch.\n",
    "1. `Prefetching` overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37bc48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "validation_ds = validation_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = validation_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa120b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6650a17",
   "metadata": {},
   "source": [
    "## Define a training strategy\n",
    "\n",
    "tf.distribute.Strategy is a TensorFlow API to distribute training across multiple GPUs, multiple machines, or TPUs. Using this API, you can distribute your existing models and training code with minimal code changes.\n",
    "- Easy to use and support multiple user segments, including researchers, machine learning engineers, etc.\n",
    "- Provide good performance out of the box.\n",
    "- Easy switching between strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ce033-27b5-4883-bcb6-52955dcfaf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display physical devices\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57faeb83-b618-485e-9112-95468a571c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display logical devices\n",
    "tf.config.list_logical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c9c56-1ed5-44fc-ba7a-21c82701625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  example, single device\n",
    "#device=\"/cpu:0\"\n",
    "\n",
    "#  example, multiple devices devices\n",
    "device=[\"/gpu:0\", \"/gpu:1\", \"/gpu:2\", \"/gpu:3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f62d4-a2b6-4f5e-900a-b244576a81bf",
   "metadata": {},
   "source": [
    "### One Device Strategy\n",
    "\n",
    "A distribution strategy for running on a single device. Device string identifier for the device on which the variables should be placed. See class docs for more details on how the device is used. Examples: \"/cpu:0\", \"/gpu:0\", \"/device:CPU:0\", \"/device:GPU:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a3be45-a5e6-4124-b431-fbf5426b396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to create a OneDeviceStrategy\n",
    "\n",
    "#strategy = tf.distribute.OneDeviceStrategy(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ed9ca",
   "metadata": {},
   "source": [
    "### Mirrored Strategy \n",
    "\n",
    "Supports synchronous distributed training on multiple GPUs on one machine. It creates one replica per GPU device. Each variable in the model is mirrored across all the replicas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda4949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to create a MirroredStrategy\n",
    "\n",
    "#strategy = tf.distribute.MirroredStrategy(devices=device)\n",
    "#strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a45f5",
   "metadata": {},
   "source": [
    "### Multi-Worker Mirrored Strategy\n",
    "\n",
    "Multi-Worker MirroredStrategy is very similar to MirroredStrategy. It implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to tf.distribute.MirroredStrategy, it creates copies of all variables in the model on each device across all workers.\n",
    "\n",
    "MultiWorkerMirroredStrategy has two implementations for cross-device communications. \n",
    "1. CommunicationImplementation.RING is RPC-based and supports both CPUs and GPUs. \n",
    "1. CommunicationImplementation.NCCL uses NCCL and provides state-of-art performance on GPUs but it doesn't support CPUs. \n",
    "1. CollectiveCommunication.AUTO defers the choice to Tensorflow. \n",
    "\n",
    "You can specify them in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b673caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to create a Multi-worker Mirrored Strategy\n",
    "\n",
    "communication_options = tf.distribute.experimental.CommunicationOptions(\n",
    "    # RING is RPC-based and supports both CPUs and GPUs.\n",
    "    #implementation=tf.distribute.experimental.CommunicationImplementation.RING)\n",
    "\n",
    "    # NCCL uses NCCL and provides state-of-art performance on GPUs but it doesn't support CPUs\n",
    "    #implementation=tf.distribute.experimental.CommunicationImplementation.NCCL)\n",
    "    \n",
    "    # AUTO defers the choice to Tensorflow.\n",
    "    implementation=tf.distribute.experimental.CommunicationImplementation.AUTO)\n",
    "\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=communication_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88fe6ca",
   "metadata": {},
   "source": [
    "## Define a model and hyperparameters\n",
    "\n",
    "When you build a model for hypertuning, you also define the hyperparameter search space in addition to the model architecture. The model you set up for hypertuning is called a hypermodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb97134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model from scratch\n",
    "! pip install keras-tuner -q\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout,Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "\n",
    "num_classes = len(class_names)\n",
    "\n",
    "inputShape=(img_height, img_width, 1)\n",
    "data_format=\"channels_last\"\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = keras.Sequential(name=\"fingerprint_prediction\")\n",
    "    data_augmentation\n",
    "    input_shape=(img_height, img_width, 1)\n",
    "    chanDim = -1\n",
    "    # first CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        hp.Int(\"conv_1\", min_value=32, max_value=96, step=32),\n",
    "        (3, 3), padding=\"same\", input_shape=inputShape, data_format=data_format))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format=data_format))\n",
    "    \n",
    "    # second CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        hp.Int(\"conv_2\", min_value=64, max_value=128, step=32),\n",
    "        (3, 3), padding=\"same\", data_format=data_format))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format=data_format))\n",
    "    \n",
    "    # third CONV => RELU => POOL layer set\n",
    "    model.add(Conv2D(\n",
    "        hp.Int(\"conv_3\", min_value=96, max_value=256, step=32),\n",
    "        (3, 3), padding=\"same\", data_format=data_format))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format=data_format))    \n",
    "    \n",
    "    # first (and only) set of FC => RELU layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hp.Int(\"dense_units\", min_value=256,\n",
    "                           max_value=768, step=256)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    # softmax classifier\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    # initialize the learning rate choices and optimizer\n",
    "    lr = hp.Choice(\"learning_rate\",\n",
    "                   values=[1e-1, 1e-2, 1e-3])\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    # return the model\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=tf.losses.CategoricalCrossentropy(from_logits=False),\n",
    "        # metrics to be evaluated by the model during training and testing.The strings 'accuracy' or 'acc', TF converts this to binary, categorical or sparse.\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16b28b8",
   "metadata": {},
   "source": [
    "## Search hyperparameters\n",
    "\n",
    "The Keras Tuner has four tuners available:\n",
    "1. RandomSearch\n",
    "1. Hyperband\n",
    "1. BayesianOptimization\n",
    "1. Sklearn. \n",
    "\n",
    "In this tutorial, you use the Hyperband tuner. \n",
    "\n",
    "The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. \n",
    "\n",
    "To instantiate the Hyperband tuner, you must specify the hypermodel, the objective to optimize and the maximum number of epochs to train (max_epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a736edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "# open a strategy scope\n",
    "tuner = kt.Hyperband(\n",
    "    model_builder,\n",
    "    objective='val_accuracy',\n",
    "    # Integer, the maximum number of epochs to train one model. It is recommended to set this to a value slightly higher than the expected epochs to convergence for your largest Model, and to use early stopping during training\n",
    "    max_epochs=5,\n",
    "    # Integer, the reduction factor for the number of epochs and number of models for each bracket. Defaults to 3.\n",
    "    factor=3,\n",
    "    # training strategy\n",
    "    distribution_strategy=strategy,\n",
    "    # directory to save the hyperparameter trials\n",
    "    # TODO Update with a variable\n",
    "    directory=scratch_path + '/tune/model_hp',\n",
    "    # folder to save the hyperparameter trail outputs\n",
    "    project_name='hypertune',\n",
    "    #  If you re-run the hyperparameter search, the Keras Tuner uses the existing state from these logs to resume the search. \n",
    "    # To disable this behavior, pass an additional overwrite=True argument while instantiating the tuner.\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12faf8",
   "metadata": {},
   "source": [
    "We’ll be using EarlyStopping to short circuit hyperparameter trials that are not performing well. Keep in mind that tuning hyperparameters is an extremely computationally expensive process, so if we can kill off poorly performing trials, we can save ourselves a bunch of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e88ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop training when a monitored metric has stopped improving\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d44fb9-c2d5-4d5e-83d6-530d04b3fcf1",
   "metadata": {},
   "source": [
    "This search will run for 10 Trials.\n",
    "- CPU: Best val_accuracy: 0.947070837020874 | Total elapsed time: 02h 11m 29s\n",
    "- GPU: Best val_accuracy So Far: 0.8727683424949646 | Total elapsed time: 00h 08m 44s\n",
    "\n",
    "The learned values will be around\n",
    "```\n",
    "[INFO] optimal number of filters in conv_1 layer: 32\n",
    "[INFO] optimal number of filters in conv_2 layer: 96\n",
    "[INFO] optimal number of filters in conv_3 layer: 160\n",
    "[INFO] optimal number of units in dense layer: 512\n",
    "[INFO] optimal learning rate: 0.0010\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_ds, epochs=4, validation_data=validation_ds, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"[INFO] optimal number of filters in conv_1 layer: {}\".format(\n",
    "\tbest_hps.get(\"conv_1\")))\n",
    "print(\"[INFO] optimal number of filters in conv_2 layer: {}\".format(\n",
    "\tbest_hps.get(\"conv_2\")))\n",
    "print(\"[INFO] optimal number of filters in conv_3 layer: {}\".format(\n",
    "\tbest_hps.get(\"conv_3\")))\n",
    "print(\"[INFO] optimal number of units in dense layer: {}\".format(\n",
    "\tbest_hps.get(\"dense_units\")))\n",
    "print(\"[INFO] optimal learning rate: {:.4f}\".format(\n",
    "\tbest_hps.get(\"learning_rate\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b91ead",
   "metadata": {},
   "source": [
    "## Fit a model\n",
    "\n",
    "Fit the model with the optimal hyperparameters and train it on the data for a desired number of epochs. This training cycle will run for 9 epochs resulting in an accuracy around ~0.98 accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88925c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=validation_ds,\n",
    "    epochs=epochs,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False\n",
    ")\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3ba56",
   "metadata": {},
   "source": [
    "## Print the model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c3ef9b",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "Should get an accuracy around 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8674afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    verbose='auto',\n",
    "    sample_weight=None,\n",
    "    steps=None,\n",
    "    callbacks=None,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe4249",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "\n",
    "A SavedModel is a directory containing serialized signatures and the state needed to run them, including variable values and vocabularies.\n",
    "\n",
    "```\n",
    "assets            directory contains files used by the TensorFlow graph\n",
    "keras_metadata.pb file  \n",
    "saved_model.pb    file stores the actual TensorFlow program, or model, and a set of named signatures for tensor I/O\n",
    "variables         directory contains a standard training checkpoint \n",
    "```\n",
    "\n",
    "There are 2 formats you can use to save an entire model to disk, the TensorFlow SavedModel format and the older Keras H5 format.\n",
    "\n",
    "For versioning, you typically generate several models made up of (code, data, config) that demands model versioning. \n",
    "\n",
    "`/parent-folder/project-name/VERSION_NUMBER/MAJOR.MINOR.PIPELINE.tf`\n",
    "\n",
    "Triton expects the following folder structure:\n",
    "\n",
    "`/parent-folder/project-name/VERSION_NUMBER/model.savedmodel`\n",
    "\n",
    "* `parent-folder` should be the root of s3 url, ex: `s3://bucket/parent-folder`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dbdc54",
   "metadata": {},
   "source": [
    "## Save locally\n",
    "\n",
    "Triton MUST have the model artifacts as `model.savedmodel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f59ffd-fef6-4940-a254-2b51672d1800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: tf.saved_model.save(model, path_to_dir)\n",
    "model_path = scratch_path + '/models/fingerprint/1/model.savedmodel'\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074c16b-9159-4917-99ea-e6ea50582ad1",
   "metadata": {},
   "source": [
    "## Convert to TensorFlow Lite model\n",
    "\n",
    "TensorFlow Lite is a set of tools that enables on-device machine learning by helping developers run their models on mobile, embedded, and edge devices.\n",
    "\n",
    "To use the trained model with on-device applications, first convert it to a smaller and more efficient model format called a TensorFlow Lite model. To continue exploration with TFLite models [see this tutorial from TensorFlow](https://www.tensorflow.org/lite/models/modify/model_maker/image_classification#simple_end-to-end_example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ff21a-9319-481c-9ae4-aef4bc5226aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_path = scratch_path + '/models/compressed/1/model.tflite'\n",
    "\n",
    "# Save the model.\n",
    "with open(tflite_path, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc59007",
   "metadata": {},
   "source": [
    "# Make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc36c87",
   "metadata": {},
   "source": [
    "## Load the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e90e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = scratch_path + '/models/fingerprint/1/model.savedmodel'\n",
    "model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f046e7",
   "metadata": {},
   "source": [
    "### Load a new fingerprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc589378",
   "metadata": {},
   "source": [
    "### Convert the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random images\n",
    "import random\n",
    "\n",
    "samples_path = scratch_path + '/real/'\n",
    "file_list = os.listdir(samples_path)\n",
    "print('Files in path: ' + str(len(file_list)))\n",
    "\n",
    "test_image = random.choice(file_list)\n",
    "print('Selected: ' + test_image)\n",
    "\n",
    "# loads an image into PIL format.\n",
    "img = tf.keras.utils.load_img(\n",
    "    samples_path + test_image,\n",
    "    color_mode=\"grayscale\",\n",
    "    target_size=(img_height, img_width),\n",
    "    interpolation='nearest',\n",
    ")\n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "\n",
    "# converts a PIL image instance to a numpy array\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "print(\"shape:\",img_array.shape)\n",
    "print(\"shape:\",img_array.dtype)\n",
    "print(\"shape:\",img_array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcc51c6",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c084f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a prediction on the new fingerprint\n",
    "predictions = model.predict(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104057a",
   "metadata": {},
   "source": [
    "# Working with S3 Buckets\n",
    "\n",
    "***ACTION REQUIRED:***\n",
    "\n",
    "Create S3 Buckets:\n",
    "- `sagemaker-fingerprint-data`\n",
    "- `sagemaker-fingerprint-models`\n",
    "\n",
    "`IMPORTANT: in order for sagemaker to write or read, the bucket names MUST include \"sagemaker-\"`\n",
    "\n",
    "### OpenShift\n",
    "\n",
    "Use CR with ACK Operator for S3 from the OpenShift console or `oc`\n",
    "\n",
    "See CRs in [openshift/examples](../../openshift/examples/)\n",
    "\n",
    "```\n",
    "oc apply -f openshift/examples/s3-sagemaker-fingerprint-data-cr.yml\n",
    "oc apply -f openshift/examples/s3-sagemaker-fingerprint-models-cr.yml\n",
    "```\n",
    "\n",
    "### AWS\n",
    "- See below [`aws` s3 cli](#aws-s3-cli)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c2219-49ed-4d46-9443-c68b15777c8e",
   "metadata": {},
   "source": [
    "## AWS S3 CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71057f1-61f1-493d-8f78-7355c888d599",
   "metadata": {},
   "source": [
    "### Create S3 Bucket\n",
    "\n",
    "Create `sagemaker-fingerprint-data` and `sagemaker-fingerprint-models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a15f15b-412e-41ea-94cd-32d489bc2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bucket\n",
    "! aws s3api create-bucket \\\n",
    "    --bucket sagemaker-fingerprint-data \\\n",
    "    --region us-east-2 \\\n",
    "    --create-bucket-configuration LocationConstraint=us-east-2\n",
    "\n",
    "! aws s3api create-bucket \\\n",
    "    --bucket sagemaker-fingerprint-models \\\n",
    "    --region us-east-2 \\\n",
    "    --create-bucket-configuration LocationConstraint=us-east-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22214a6-968f-4a63-8b24-04086b35f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list objects using the aws s3 cli\n",
    "! aws s3 ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e855be42-785d-4cc7-9352-0b7f36859322",
   "metadata": {},
   "source": [
    "### Upload training data to s3\n",
    "\n",
    "1. Upload left data from `scratch` to `s3://sagemaker-fingerprint-data/left`\n",
    "1. Upload right data from `scratch` to `s3://sagemaker-fingerprint-data/right`\n",
    "1. Upload real data from `scratch` to `s3://sagemaker-fingerprint-data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b733d1-8f8f-486a-a9e0-8264321315a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the objects to s3 demonstrating interaction with object storage\n",
    "! aws s3 sync ${SCRATCH}/train/left s3://sagemaker-fingerprint-data/train/left --quiet && \\\n",
    "  aws s3 sync ${SCRATCH}/train/right s3://sagemaker-fingerprint-data/train/right --quiet && \\\n",
    "  aws s3 sync ${SCRATCH}/real s3://sagemaker-fingerprint-data/real --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18047be-9dc0-4b3e-b60c-1f2dd36bdc7a",
   "metadata": {},
   "source": [
    "### Download training data from s3\n",
    "\n",
    "1. Download left data from `s3://sagemaker-fingerprint-data/left` to `train/left`\n",
    "1. Download right data from `s3://sagemaker-fingerprint-data/right` to `train/right`\n",
    "1. Download real data from `s3://sagemaker-fingerprint-data`  to `real`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffbc157-1ca2-43c2-8d28-04fbf1598dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the objects to s3 demonstrating interaction with object storage\n",
    "! aws s3 sync s3://sagemaker-fingerprint-data/train/left $SCRATCH/train/left --quiet && \\\n",
    " aws s3 sync s3://sagemaker-fingerprint-data/train/right $SCRATCH/train/right --quiet && \\\n",
    " aws s3 sync s3://sagemaker-fingerprint-data/real $SCRATCH/real --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60023558",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload tested models to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14928535-7f5a-4fa5-8b6b-4e0aa7c138ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the models to S3\n",
    "! aws s3 sync ${SCRATCH}/models s3://sagemaker-fingerprint-models/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaf1586",
   "metadata": {},
   "source": [
    "### Boto3 usage\n",
    "\n",
    "Boto3 allows for easy access to AWS resources. We will list the s3 buckets available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0168d92-cfd5-4444-b9cb-252cceaac5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boto3 library allows for easy access to aws ecosystem of tools and products\n",
    "# SageMaker is a part of aws ecosystem of tools, so it allows easy access to S3\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# list all available buckets\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acbffbe-083e-4076-a5a9-6807998b34b9",
   "metadata": {},
   "source": [
    "# Cleanup artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417ec1c-dfc0-4f2f-80b8-0a0944bc0e78",
   "metadata": {},
   "source": [
    "## Delete local directory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de13571-bdbd-4e94-9e5d-9fe39eda9090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! rm -rf $scratch_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fbaf16404a186558c5830ba9c1870690eb62baaca2b916af320bab80c6b3e76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
